{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "plant disease classification",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNVKz4QCzl2K",
        "colab_type": "code",
        "outputId": "1b0e12bd-91f8-4579-8729-740b8a08e210",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9UEgV1J1PZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## library import\n",
        "%matplotlib inline\n",
        "\n",
        "# for image\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39uO9VWK1szR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f87MlZrg1ukK",
        "colab_type": "code",
        "outputId": "f16c1d8e-c6b7-472d-c133-0e36116c2c0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efRfzXNs6H8Q",
        "colab_type": "code",
        "outputId": "f1ac2f7e-88e7-4e4a-8f88-6654cc5757e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LszLUCtZ6J4V",
        "colab_type": "code",
        "outputId": "6ca125c1-2a4b-4a7e-c965-d222bae20a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mXIw-_q7GpD",
        "colab_type": "text"
      },
      "source": [
        "### Data Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZr12RvM6Sir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dir = 'gdrive/My Drive/Colab Notebooks/data/train/train_dir'\n",
        "test_dir = 'gdrive/My Drive/Colab Notebooks/data/test'\n",
        "train_files = os.listdir(train_dir)\n",
        "test_files = os.listdir(test_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5mY1fSDGPZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# subset of the dataset to make faster to train\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "training_samples = 300\n",
        "test_samples = 300\n",
        "\n",
        "train_sampler = SubsetRandomSampler(np.arange(training_samples, dtype=np.int64))\n",
        "test_sampler = SubsetRandomSampler(np.arange(test_samples, dtype=np.int64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcwy6es4GZ4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For traing data we used Data Augmentation of the form of transforms that\n",
        "# randomly rotate, scale and crop, then flip images\n",
        "trans = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "# For test data we don't use any form of Data Augmentation and just used\n",
        "# resize and center crop to adjust our images for input\n",
        "\n",
        "\n",
        "test_trans = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "# Define train and test folders located on google drive\n",
        "train_data = torchvision.datasets.ImageFolder(root=train_dir, transform=trans)\n",
        "\n",
        "test_data = torchvision.datasets.ImageFolder(root=test_dir, transform=test_trans)\n",
        "\n",
        "#Define dataloaders\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64,\n",
        "                                          sampler=train_sampler, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=64,\n",
        "                                          sampler=test_sampler, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsE022kdGe7I",
        "colab_type": "code",
        "outputId": "d879588a-bd25-450a-80f0-6fa1ae533327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#폴더명을 label로 알려줌\n",
        "print(train_data.classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato__Target_Spot', 'Tomato__Tomato_YellowLeaf__Curl_Virus', 'Tomato__Tomato_mosaic_virus', 'Tomato_healthy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMwcLXAm7JR_",
        "colab_type": "text"
      },
      "source": [
        "### Building Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg4TGW547OtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models\n",
        "net = models.vgg19().to(device)\n",
        "#net = models.resnet50().to(device)\n",
        "# net = PreActResNet18()\n",
        "# net = GoogLeNet()\n",
        "# net = DenseNet121()\n",
        "# net = ResNeXt29_2x64d()\n",
        "# net = MobileNet()\n",
        "# net = MobileNetV2()\n",
        "# net = DPN92()\n",
        "# net = ShuffleNetG2()\n",
        "# net = SENet18()\n",
        "# net = ShuffleNetV2(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q0iblRV7USt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "# Transfer learning 사용으로 classifier만 training\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = True\n",
        "num_ftrs = net.classifier[0].in_features\n",
        "\n",
        "\n",
        "# Replace the last fully-connected layer\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "net.fc = nn.Linear(num_ftrs, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfJF5FJ07WJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized\n",
        "#optimizer_conv = optim.SGD(net.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "# 모든 매개변수들이 최적화되었는지 관찰\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)\n",
        "\n",
        "# 5 에폭마다 0.1씩 학습율 감소\n",
        "lr_sche = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMXi-jTp7YUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(epoch):\n",
        "    # print(len(trainloader))\n",
        "    # loop over the dataset multiple times\n",
        "      net.train()\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "      lr_sche.step()\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs\n",
        "          inputs, labels = data\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = net(inputs)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          loss = criterion(outputs, labels)\n",
        "          \n",
        "          # 역전파 + 최적화\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item() * inputs.size(0)\n",
        "          running_corrects += torch.sum(preds == labels.data)\n",
        "          \n",
        "      epoch_loss = running_loss / len(trainloader)\n",
        "      epoch_acc = running_corrects.double() / len(trainloader)\n",
        "\n",
        "      print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                'train', epoch_loss, epoch_acc))\n",
        " \n",
        "\n",
        "      print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_zAqYcU7aTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    running_corrects = 0\n",
        "    with torch.no_grad():\n",
        "        for i,data in enumerate(testloader):\n",
        "            inputs, labels = data\n",
        "            inputs, targets = inputs.to(device), labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            \n",
        "        epoch_loss = running_loss / len(testloader)\n",
        "        epoch_acc = running_corrects.double() / len(testloader)\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                'test', epoch_loss, epoch_acc))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*epoch_acc\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idjGYQ077fcA",
        "colab_type": "code",
        "outputId": "6600d5cc-fc6b-4e72-9d54-93234d972afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "source": [
        "import copy\n",
        "start_epoch = 0\n",
        "net = net.to(device)\n",
        "for epoch in range(start_epoch, start_epoch+25):\n",
        "  best_model_wts = copy.deepcopy(net.state_dict())\n",
        "  best_acc = 0.0\n",
        "  train_model(epoch)\n",
        "  test_model(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train Loss: 299.3606 Acc: 37.0000\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-64d4381985a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-59-377300f8d777>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KeyError:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 99, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 137, in __getitem__\n    path, target = self.samples[index]\nIndexError: list index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f08ZXWoEgsNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}